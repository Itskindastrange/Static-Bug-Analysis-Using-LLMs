{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FAST\\.conda\\envs\\degradeation\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\FAST\\.conda\\envs\\degradeation\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\FAST\\.conda\\envs\\degradeation\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_vul_classes, num_danger_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.vul_classifier = nn.Linear(self.bert.config.hidden_size, num_vul_classes)  # Multi-label output\n",
    "        self.danger_classifier = nn.Linear(self.bert.config.hidden_size, num_danger_classes)  # Single-label output\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token output\n",
    "        vul_logits = self.vul_classifier(cls_output)\n",
    "        danger_logits = self.danger_classifier(cls_output)\n",
    "        return vul_logits, danger_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model with the correct number of classes\n",
    "num_vul_classes = 8 # Replace with your actual number of vulnerability classes\n",
    "num_danger_classes = 4  # Replace with your actual number of danger level classes\n",
    "model = BertClassifier(num_vul_classes, num_danger_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Load the saved model state\n",
    "checkpoint = torch.load(\"final_bert_classifier.pkl\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Load the encoders\n",
    "mlb = joblib.load(\"mlb.pkl\")  # Path to saved MultiLabelBinarizer\n",
    "le_danger = joblib.load(\"le_danger.pkl\")  # Path to saved LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_details(text, model, tokenizer, dataset, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict the vulnerability type and danger level for the given text and retrieve additional details.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer(\n",
    "        text,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get model predictions\n",
    "        vul_logits, danger_logits = model(tokens['input_ids'], tokens['attention_mask'])\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        vul_probs = torch.sigmoid(vul_logits).cpu().numpy()[0]\n",
    "        danger_probs = torch.softmax(danger_logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        # Debug: Log probabilities\n",
    "        #print(f\"Vulnerability Probabilities: {vul_probs}\")\n",
    "        #print(f\"Danger Level Probabilities: {danger_probs}\")\n",
    "\n",
    "        # Multi-label vulnerabilities above the threshold\n",
    "        vul_labels = [mlb.classes_[i] for i, prob in enumerate(vul_probs) if prob > threshold]\n",
    "        print(f\"Detected Vulnerabilities: {vul_labels}\")\n",
    "\n",
    "        # Single-label danger level (highest probability)\n",
    "        danger_label = le_danger.classes_[np.argmax(danger_probs)]\n",
    "\n",
    "    # Match predicted vulnerabilities and danger levels to the dataset\n",
    "    predictions = {\n",
    "        \"vulnerability_type\": vul_labels,\n",
    "        \"danger_level\": danger_label,\n",
    "        \"description\": [],\n",
    "        \"fix_suggestions\": []\n",
    "    }\n",
    "\n",
    "    for vul in vul_labels:\n",
    "        # Find matching rows in the dataset\n",
    "        match = dataset[dataset['vulnerability_type'].str.contains(vul, regex=False)]\n",
    "        if not match.empty:\n",
    "            # Append descriptions and fixes\n",
    "            predictions[\"description\"].append(match.iloc[0]['description'])\n",
    "            predictions[\"fix_suggestions\"].extend(match.iloc[0]['fix_suggestions'])\n",
    "        else:\n",
    "            print(f\"No match found in dataset for vulnerability: {vul}\")\n",
    "\n",
    "    # Remove duplicates in suggestions\n",
    "    predictions[\"fix_suggestions\"] = list(set(predictions[\"fix_suggestions\"]))\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"data_big.json\")  # Replace with your dataset path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the code snippet to analyze (press Enter twice to finish):\n",
      "Detected Vulnerabilities: ['Cross-Site Request Forgery (CSRF)', 'DOM-Based XSS', 'Improper Error Handling', 'Improper Input Validation', 'Insecure Deserialization', 'Reflected XSS in URL Parameters', 'SQL Injection', 'Stored XSS']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo input provided. Exiting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Get predictions with details\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_with_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Predicted Results ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 33\u001b[0m, in \u001b[0;36mpredict_with_details\u001b[1;34m(text, model, tokenizer, dataset, threshold)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Vulnerabilities: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvul_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Single-label danger level (highest probability)\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     danger_label \u001b[38;5;241m=\u001b[39m le_danger\u001b[38;5;241m.\u001b[39mclasses_[\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39margmax(danger_probs)]\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Match predicted vulnerabilities and danger levels to the dataset\u001b[39;00m\n\u001b[0;32m     36\u001b[0m predictions \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvulnerability_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: vul_labels,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdanger_level\u001b[39m\u001b[38;5;124m\"\u001b[39m: danger_label,\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfix_suggestions\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m     41\u001b[0m }\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def get_user_input():\n",
    "    \"\"\"\n",
    "    Prompt the user to input a code snippet, line by line, and return the complete input as a string.\n",
    "    \"\"\"\n",
    "    print(\"Enter the code snippet to analyze (press Enter twice to finish):\")\n",
    "    user_input_lines = []\n",
    "    while True:\n",
    "        line = input()\n",
    "        if line == \"\":\n",
    "            break\n",
    "        user_input_lines.append(line)\n",
    "    return \"\\n\".join(user_input_lines)\n",
    "\n",
    "# Main logic for prediction\n",
    "if __name__ == \"__main__\":\n",
    "    # Prompt the user for input\n",
    "    input_text = get_user_input()\n",
    "\n",
    "    # Ensure the user provided some input\n",
    "    if not input_text.strip():\n",
    "        print(\"No input provided. Exiting.\")\n",
    "    else:\n",
    "        # Get predictions with details\n",
    "        result = predict_with_details(input_text, model, tokenizer, df, threshold=0.05)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"\\n=== Predicted Results ===\")\n",
    "        print(f\"Vulnerability Type: {result['vulnerability_type']}\")\n",
    "        print(f\"Danger Level: {result['danger_level']}\")\n",
    "        print(\"\\nDescription of Vulnerabilities:\")\n",
    "        for description in result[\"description\"]:\n",
    "            print(f\"- {description}\")\n",
    "        print(\"\\nFix Suggestions:\")\n",
    "        for suggestion in result[\"fix_suggestions\"]:\n",
    "            print(f\"- {suggestion}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degradeation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
